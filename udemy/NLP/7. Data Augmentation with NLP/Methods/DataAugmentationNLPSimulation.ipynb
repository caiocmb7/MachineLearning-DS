{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataAugmentationNLPSimulation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "GDivOCypwHni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr9i-rrvvgWp",
        "outputId": "5201591d-89ed-4ca3-a556-a9a1d7320baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textattack\n",
            "  Downloading textattack-0.3.4-py3-none-any.whl (373 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 61 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 71 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 92 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 122 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 133 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 143 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 153 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 174 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 184 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 194 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 204 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 215 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 225 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 235 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 245 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 256 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 266 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 276 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 286 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 296 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 307 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 317 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 327 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 337 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 348 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 358 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 368 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 373 kB 24.3 MB/s \n",
            "\u001b[?25hCollecting flair\n",
            "  Downloading flair-0.10-py3-none-any.whl (322 kB)\n",
            "\u001b[K     |████████████████████████████████| 322 kB 68.7 MB/s \n",
            "\u001b[?25hCollecting bert-score>=0.3.5\n",
            "  Downloading bert_score-0.3.11-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.7.1)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.6 MB/s \n",
            "\u001b[?25hCollecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting lru-dict\n",
            "  Downloading lru-dict-1.1.7.tar.gz (10 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from textattack) (0.5.3)\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from textattack) (3.2.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from textattack) (8.12.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.10.0+cu111)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from textattack) (3.6.0)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.21.5)\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.0-py3-none-any.whl (34 kB)\n",
            "Collecting lemminflect\n",
            "  Downloading lemminflect-0.2.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 36.2 MB/s \n",
            "\u001b[?25hCollecting transformers>=3.3.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.4.1)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->bert-score>=0.3.5->textattack) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->textattack) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.10.0.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 41.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack) (4.11.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.8 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 55.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.2-py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 69.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.1-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 54.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.0-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 57.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 56.0 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 58.1 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.16.0-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 54.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 55.7 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.15.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 52.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 32.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 45.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 12.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 45.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.0-py3-none-any.whl (285 kB)\n",
            "\u001b[K     |████████████████████████████████| 285 kB 13.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 45.0 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.12.0-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 44.7 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (0.3.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (6.0.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (0.70.12.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 44.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.10.1-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 45.0 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.10.0-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 37.8 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.9.0-py3-none-any.whl (262 kB)\n",
            "\u001b[K     |████████████████████████████████| 262 kB 45.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 41.4 MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 55.3 MB/s \n",
            "\u001b[?25hCollecting pyarrow<4.0.0,>=1.0.0\n",
            "  Downloading pyarrow-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (20.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.7 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.7.0-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 80.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 57.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.1-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 59.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.0-py3-none-any.whl (202 kB)\n",
            "\u001b[K     |████████████████████████████████| 202 kB 45.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.5.0-py3-none-any.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 57.0 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 58.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.4.0-py3-none-any.whl (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 72.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 57.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.0.4)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 59.3 MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 74.2 MB/s \n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 27.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair->textattack) (0.8.9)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 67.7 MB/s \n",
            "\u001b[?25hCollecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting more-itertools\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair->textattack) (4.2.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair->textattack) (1.0.2)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair->textattack) (3.6.0)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair->textattack) (1.13.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair->textattack) (5.2.1)\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.3.0->textattack) (3.7.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack) (2.0.12)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair->textattack) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair->textattack) (3.1.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair->textattack) (0.2.5)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words->textattack) (0.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.3.0->textattack) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, sqlitedict, langdetect, lru-dict, wikipedia-api, word2number\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9692 sha256=df533fe9fe8668f8c05a4a777799d79e532e068e44eb57e8a412779c4ce3e3f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=744368c26d2f84abaf8ab26d3ae467c0313ca0f5181a7cc19f0f92e175d22cfc\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=ba1a97d33a2e718a91b4f6af4a9a94632744c68914cec01955c642944789ca78\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15736 sha256=72634dcf55890090765bf0cffd0aea194de475b3d82d78c66aa9c73786b40691\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=558529e50369ccfe759553abe35a613a108da3697160cd28cbb5eb9965366f4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for lru-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lru-dict: filename=lru_dict-1.1.7-cp37-cp37m-linux_x86_64.whl size=28431 sha256=72e99bf8d86d8cb1ffa765c10de5c6e340798a3b2ed87877aff602bffcaeb3a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/0b/4e/aa8fec9833090cd52bcd76f92f9d95e1ee7b915c12093663b4\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=1e08b7ef6ace67d9a609fcb45ed5109266b9cb00898791a4a7ad984f7dd2d642\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=0dff33dd4e9db2528c4bdf1e57ac6ba6022ac6693313b6b03fa8f13c4aae1ebd\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built gdown mpld3 overrides sqlitedict langdetect lru-dict wikipedia-api word2number\n",
            "Installing collected packages: tqdm, requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, sacremoses, overrides, huggingface-hub, xxhash, wikipedia-api, transformers, sqlitedict, segtok, mpld3, more-itertools, langdetect, konoha, janome, gdown, ftfy, deprecated, conllu, bpemb, word2number, terminaltables, num2words, lru-dict, lemminflect, language-tool-python, flair, datasets, bert-score, textattack\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.63.0\n",
            "    Uninstalling tqdm-4.63.0:\n",
            "      Successfully uninstalled tqdm-4.63.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.2\n",
            "    Uninstalling importlib-metadata-4.11.2:\n",
            "      Successfully uninstalled importlib-metadata-4.11.2\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bert-score-0.3.11 bpemb-0.3.3 conllu-4.4.1 datasets-1.2.1 deprecated-1.2.13 flair-0.10 ftfy-6.1.1 gdown-3.12.2 huggingface-hub-0.4.0 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 language-tool-python-2.7.0 lemminflect-0.2.2 lru-dict-1.1.7 more-itertools-8.8.0 mpld3-0.3 num2words-0.5.10 overrides-3.1.0 pyyaml-6.0 requests-2.27.1 sacremoses-0.0.47 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 terminaltables-3.1.10 textattack-0.3.4 tokenizers-0.11.6 tqdm-4.49.0 transformers-4.17.0 wikipedia-api-0.5.4 word2number-1.1 xxhash-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install textattack"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_jFwVrQwLPA",
        "outputId": "52b98f36-8edc-46a0-de0b-837fff815cd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 19.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.13.3)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 66.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.24.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.44.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (13.0.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.21.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.3.1)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.2.0)\n",
            "Installing collected packages: importlib-metadata, tf-estimator-nightly, tensorflow-text\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 3.10.1\n",
            "    Uninstalling importlib-metadata-3.10.1:\n",
            "      Successfully uninstalled importlib-metadata-3.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "konoha 4.6.5 requires importlib-metadata<4.0.0,>=3.7.0, but you have importlib-metadata 4.11.3 which is incompatible.\u001b[0m\n",
            "Successfully installed importlib-metadata-4.11.3 tensorflow-text-2.8.1 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textaugment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix6yRHe_wcj_",
        "outputId": "e29be9f1-29bb-45b8-eb23-0f2572b9845a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textaugment\n",
            "  Downloading textaugment-1.3.4-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from textaugment) (3.6.0)\n",
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from textaugment) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from textaugment) (1.21.5)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from textaugment) (0.15.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->textaugment) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->textaugment) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->textaugment) (1.4.1)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2.10)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans->textaugment) (3.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2021.10.8)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2021.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 47.5 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15735 sha256=6a1f0b0c5e4a0aeccbb344867e0fcbf1982bee9b8e179acdccbc70b821dfdcc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/da/eb/a54579056f265eede0417df537dd56d3df5b9eb2b25df0003d\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans, textaugment\n",
            "Successfully installed googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0 textaugment-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy requests nlpaug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD8lKMCFwe74",
        "outputId": "da033551-5d79-4fff-e87a-0f73ef7e3feb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.27.1)\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.10-py3-none-any.whl (410 kB)\n",
            "\u001b[K     |████████████████████████████████| 410 kB 22.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->nlpaug) (1.15.0)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24mboaC8wiAn",
        "outputId": "c36e7e86-ee07-4b0c-f3be-1d3df69436a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.49.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3QFFUSRNFVk",
        "outputId": "2c141f64-1127-47c4-f985-cf40308bc2c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=34eb7c702a4d66405c7f7cf63dbfe1656a12073f6e5d9bfd421f5b28c99a8892\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "from nlpaug.util import Action"
      ],
      "metadata": {
        "id": "2EKRvtZm35pq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "QtWuX9i82wpU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGebIkSw2x-T",
        "outputId": "736d6fc3-a20d-4cda-8b2b-a8954c097503"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://www.github.com/GEM-benchmark/NL-Augmenter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMXJ7n8cFW2L",
        "outputId": "7734e06c-020e-4ab1-f3f4-502c4043f8c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NL-Augmenter'...\n",
            "warning: redirecting to https://github.com/GEM-benchmark/NL-Augmenter.git/\n",
            "remote: Enumerating objects: 12831, done.\u001b[K\n",
            "remote: Counting objects: 100% (1502/1502), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1122/1122), done.\u001b[K\n",
            "remote: Total 12831 (delta 764), reused 1004 (delta 373), pack-reused 11329\u001b[K\n",
            "Receiving objects: 100% (12831/12831), 92.97 MiB | 21.57 MiB/s, done.\n",
            "Resolving deltas: 100% (8212/8212), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd NL-Augmenter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4da7-QVFayG",
        "outputId": "56b8fff0-3bfc-4392-c2f0-d81c2de68713"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NL-Augmenter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --quiet\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlWnOyysFcNn",
        "outputId": "3f7bf149-7804-4be6-bac2-741f4256d484"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 12.1 MB 19.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 46.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 280 kB 53.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 51.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 59.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 190 kB 74.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 105 kB 42.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 10.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 167 kB 68.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 51.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 56.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 652 kB 58.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 207 kB 47.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 32.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 749 kB 65.4 MB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/typed-ast/\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 843 kB 68.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 98 kB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 48.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 41 kB 549 kB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 461 kB 57.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 52.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 41.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 419 kB 58.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 104 kB 53.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 61.1 MB/s \n",
            "\u001b[?25h  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires coverage==3.7.1, but you have coverage 6.3.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "coveralls 0.5 requires coverage<3.999,>=3.6, but you have coverage 6.3.2 which is incompatible.\u001b[0m\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz (13.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7 MB 18.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.21.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.27.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.49.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.14)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.9.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.10.8)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-3.0.0-py3-none-any.whl size=13704318 sha256=ec87275c4fd1b1cc063a20f73132bfaf69c49048922ec7941e4110a2b938ab82\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/86/ba/c33ceff8af5cb8a963e86131912039d39b37227f9787661bca\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulating categorical values"
      ],
      "metadata": {
        "id": "7rImcWhpAaw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando palavras categóricas a partir de uma lista determinada"
      ],
      "metadata": {
        "id": "Ro3H15o3Ez1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reserved_words = [\n",
        "                   [\"big\", \"not too big\", \"colossal\", \"gigantic\", \"large\", \"substantial\", \"spacious\", \"immense\", \"mammoth\", \"grand\", \"gargantuan\", \"stupendous\"],\n",
        "]\n",
        "categorical_values = \"big\"\n",
        "aug = naw.ReservedAug(reserved_tokens=reserved_words)\n",
        "augmented_categorical = aug.augment(categorical_values, n = 10)\n",
        "print(f\"Original categorical:\\n {categorical_values}\")\n",
        "print(f\"Augmented categorical:\\n {augmented_categorical}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxZdmMhtAdM5",
        "outputId": "249fee09-3ec7-448c-9a03-f0e608003698"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original categorical:\n",
            " big\n",
            "Augmented categorical:\n",
            " ['colossal', 'not too big', 'grand', 'stupendous', 'grand', 'colossal', 'gigantic', 'not too big', 'large', 'gargantuan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mudando o nome da pessoa"
      ],
      "metadata": {
        "id": "ZufUsKyME4ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformations.change_person_named_entities.transformation import ChangePersonNamedEntities\n",
        "\n",
        "aug = ChangePersonNamedEntities(n = 1, max_outputs = 9)\n",
        "aug.generate(\"Michael\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A57DBNLE4SY",
        "outputId": "882fbe8f-7488-4ea3-9cd0-9a768199c029"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Austin', 'Scott', 'Juan', 'Luke', 'Luke', 'William', 'Nathaniel', 'Kevin']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformations.entity_mention_replacement_ner.transformation import EntityMentionReplacementNER\n",
        "\n",
        "aug = EntityMentionReplacementNER(token_sequence = [\"Larry Page developed PageRank at Stanford University in 1996 as part of a research project .\"],\n",
        "        tag_sequence= [\"B-PER I-PER O O O B-ORG I-ORG O O O O O O O O O\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "ynPajz7pUzoK",
        "outputId": "11e23c2c-1965-45a1-bc03-8fe394ff7036"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-13eccfdce95d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m aug = EntityMentionReplacementNER(token_sequence = [\"Larry Page developed PageRank at Stanford University in 1996 as part of a research project .\"],\n\u001b[0;32m----> 4\u001b[0;31m         tag_sequence= [\"B-PER I-PER O O O B-ORG I-ORG O O O O O O O O O\"])\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'token_sequence'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulating e-mails"
      ],
      "metadata": {
        "id": "CBemSl_t2ISn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# problema quando se usa @gmail.com, dá UNK\n",
        "\n",
        "email = \"john_rashford@dell.com\"\n",
        "email2 = \"greg_antony@yahoo.com\"\n",
        "emails = [\"john_rashford@outlook.com\", \"greg_antony@yahoo.com\", \"michael_grimm@dell.com\"]"
      ],
      "metadata": {
        "id": "8IW_FY282Knl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando emails a partir de um contexto dado"
      ],
      "metadata": {
        "id": "ji3mF7X6HHHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action = \"substitute\", stopwords = [\"@outlook.com\", \"@gmail.com\", \"@dell.com\", \"outlook\", \"com\", \".\", \" \", \"dell\", \"gmail\"]) \n",
        "augmented_email = aug.augment(email, n = 5)\n",
        "print(f\"Original email:\\n {email}\")\n",
        "print(f\"Augmented email:\\n {augmented_email}\")\n",
        "res = []\n",
        "for ele in augmented_email:\n",
        "    j = ele.replace(' ', '')\n",
        "    res.append(j)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IqOXKAV6Xtv",
        "outputId": "9b9768a4-3c14-4b20-8dfd-6166e4d83856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original email:\n",
            " john_rashford@dell.com\n",
            "Augmented email:\n",
            " ['craig _ ellis @ dell. com', 'sean _ graham @ dell. com', 'mary _ moore @ dell. com', 'neil _ andrews @ dell. com', 'dana _ allen @ dell. com']\n",
            "['craig_ellis@dell.com', 'sean_graham@dell.com', 'mary_moore@dell.com', 'neil_andrews@dell.com', 'dana_allen@dell.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando emails a partir de um email já existente no banco de dados"
      ],
      "metadata": {
        "id": "IC2GWRlvqb8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action = \"substitute\", stopwords = [\"@outlook.com\", \"@gmail.com\", \"@dell.com\", \"outlook\", \"com\", \".\", \" \", \"dell\", \"gmail\"]) \n",
        "augmented_email = aug.augment(email, n = 5)\n",
        "print(f\"Original email:\\n {email}\")\n",
        "res = []\n",
        "for ele in augmented_email:\n",
        "    j = ele.replace(' ', '')\n",
        "    res.append(j)\n",
        "\n",
        "print(f\"Augmented email:\\n {res}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8np7sTnvRkuK",
        "outputId": "cf9c00f8-0984-45a2-dcdd-620a80cc9602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original email:\n",
            " john_rashford@dell.com\n",
            "Augmented email:\n",
            " ['steve_wilson@dell.com', 'william_wilson@dell.com', 'arthur_morgan@dell.com', 'mary_doe@dell.com', 'al_wayne@dell.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action = \"substitute\", stopwords = [\"@outlook.com\", \"@dell.com\", \"outlook\", \"com\", \".\", \" \", \"dell\", \"yahoo\"]) \n",
        "augmented_email2 = aug.augment(email2, n = 5)\n",
        "print(f\"Original email:\\n {email2}\")\n",
        "res = []\n",
        "for ele in augmented_email2:\n",
        "    j = ele.replace(' ', '')\n",
        "    res.append(j)\n",
        "\n",
        "print(f\"Augmented email:\\n {res}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k90pv3IYbG3T",
        "outputId": "c2123654-ef18-4fd3-b64b-9c326b60a16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original email:\n",
            " greg_antony@yahoo.com\n",
            "Augmented email:\n",
            " ['edward_evans@yahoo.com', 'david_wilson@yahoo.com', 'jerry_lewis@yahoo.com', 'alpha_greg@yahoo.com', 'robert_todd@yahoo.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action = \"substitute\", stopwords = [\"@outlook.com\", \"@gmail.com\", \"@dell.com\", \"outlook\", \"com\", \".\", \" \", \"dell\", \"gmail\", \"yahoo\"]) \n",
        "augmented_emails = aug.augment(emails, n = 10)\n",
        "print(f\"Original email:\\n {emails}\")\n",
        "res = []\n",
        "for ele in augmented_emails:\n",
        "    j = ele.replace(' ', '')\n",
        "    res.append(j)\n",
        "\n",
        "print(f\"Augmented email:\\n {res}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWUcOWtCTEom",
        "outputId": "e1650756-87a8-405a-b76b-b182b10df25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original email:\n",
            " ['john_rashford@outlook.com', 'greg_antony@yahoo.com', 'michael_grimm@dell.com']\n",
            "Augmented email:\n",
            " ['brad_ellis@outlook.com', 'alex_brown@yahoo.com', 'scott_white@dell.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUpf2yuwmLoL",
        "outputId": "e2c08800-ab38-445a-f87c-799231c20891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting names\n",
            "  Downloading names-0.3.0.tar.gz (789 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 28.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 40 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 51 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 61 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 81 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 92 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 102 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 112 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 122 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 133 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 143 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 153 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 163 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 174 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 184 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 194 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 204 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 215 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 225 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 235 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 245 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 256 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 266 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 276 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 286 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 296 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 307 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 317 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 327 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 337 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 348 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 358 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 368 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 378 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 389 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 399 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 409 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 419 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 430 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 440 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 450 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 460 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 471 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 481 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 491 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 501 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 512 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 522 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 532 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 542 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 552 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 563 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 573 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 583 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 593 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 604 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 614 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 624 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 634 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 645 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 655 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 665 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 675 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 686 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 696 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 706 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 716 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 727 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 737 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 747 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 757 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 768 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 778 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 788 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 789 kB 25.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: names\n",
            "  Building wheel for names (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for names: filename=names-0.3.0-py3-none-any.whl size=803699 sha256=b45b2cc801be737215525e23bbe1e46573e9bddff2f8898e9568e8c3b02da102\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ea/68/92f6b0669e478af9b7c3c524520d03050089e034edcc775c2b\n",
            "Successfully built names\n",
            "Installing collected packages: names\n",
            "Successfully installed names-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando emails aleatoriamente"
      ],
      "metadata": {
        "id": "i8Co_DThHVWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import names\n",
        "base = \"@dell.com\"\n",
        "aleatory_email = (names.get_last_name()).lower() + base\n",
        "aug = nac.RandomCharAug(action = \"insert\", stopwords = [\"dell\", \".\", \"com\", \"@\"], include_upper_case=False, spec_char = \"\")\n",
        "augmented_aleatory_email = aug.augment(aleatory_email, n = 5)\n",
        "print(f\"Original email:\\n {aleatory_email}\")\n",
        "print(f\"Original email:\\n {augmented_aleatory_email}\")\n",
        "res = []\n",
        "for ele in augmented_aleatory_email:\n",
        "    j = ele.replace(' ', '')\n",
        "    res.append(j)\n",
        "\n",
        "print(f\"Augmented email:\\n {res}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-4tjDcodpg6",
        "outputId": "2674e077-6739-4e39-cd89-5abc7c5d4c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original email:\n",
            " clark@dell.com\n",
            "Original email:\n",
            " ['cvlarvk @ dell. com', 'clairgk @ dell. com', 'kclarsk @ dell. com', 'clua7rk @ dell. com', 'cleairk @ dell. com']\n",
            "Augmented email:\n",
            " ['cvlarvk@dell.com', 'clairgk@dell.com', 'kclarsk@dell.com', 'clua7rk@dell.com', 'cleairk@dell.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A ideia é gerar um email aleatorio e a partir desse email aleatorio, gerar outros emails aleatórios"
      ],
      "metadata": {
        "id": "9826TpyKqPyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base = \"@dell.com\"\n",
        "aleatory_email = (names.get_last_name()).lower() + base\n",
        "aug = nafc.Sequential(\n",
        "    flow = [nac.RandomCharAug(action = \"insert\", stopwords = [\"dell\", \".\", \"com\", \"@\"], include_upper_case=False, spec_char = \"\"),\n",
        "            nac.RandomCharAug(action = \"delete\", stopwords = [\"dell\", \".\", \"com\", \"@\"], include_upper_case=False, spec_char = \"\"),\n",
        "            nac.RandomCharAug(action = \"substitute\", stopwords = [\"dell\", \".\", \"com\", \"@\"], include_upper_case=False, spec_char = \"\")]\n",
        ")\n",
        "aug = nac.RandomCharAug(action = \"insert\", stopwords = [\"dell\", \".\", \"com\", \"@\"], include_upper_case=False, spec_char = \"\")\n",
        "augmented_aleatory_email = aug.augment(aleatory_email, n = 5)\n",
        "print(f\"Generate aleatory email:\\n {aleatory_email}\")\n",
        "res = []\n",
        "for ele in augmented_aleatory_email:\n",
        "    j = ele.replace(' ', '')\n",
        "    res.append(j)\n",
        "\n",
        "print(f\"Augmented email:\\n {res}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exo8y5ZYpn6w",
        "outputId": "fdcceec6-2813-4ef8-9e10-ee15a390bae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate aleatory email:\n",
            " lyles@dell.com\n",
            "Augmented email:\n",
            " ['cly1les@dell.com', 'lylbess@dell.com', 'rlyleys@dell.com', 'lyvle4s@dell.com', 'wlyl4es@dell.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulating numerical values"
      ],
      "metadata": {
        "id": "hUlS9mWc5e5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "\n",
        "dt_min = datetime(2014, 10, 19)\n",
        "dt_max = datetime(2015, 10, 19)\n",
        "\n",
        "dt_min = dt_min.replace(tzinfo=timezone.utc).timestamp()\n",
        "dt_max = dt_max.replace(tzinfo=timezone.utc).timestamp()\n",
        "\n",
        "for timestamp in np.random.randint(low=dt_min, high=dt_max, size=5):\n",
        "  date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
        "  print(date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4nrGGwwZdBQ",
        "outputId": "ed1126ab-0dfb-499c-c834-763739c0fde5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2015-02-03 08:44:01\n",
            "2015-04-22 00:04:53\n",
            "2014-10-26 00:32:25\n",
            "2014-11-23 23:14:21\n",
            "2014-12-30 12:15:04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLd6aeutLFZE",
        "outputId": "3686ced8-0835-4737-de32-9f2f08c2c871"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2014-12-30 12:15:04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformations.replace_numerical_values.transformation import ReplaceNumericalValues\n",
        "\n",
        "aug = ReplaceNumericalValues()\n",
        "\n",
        "aug.generate(\"2034\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afbsd0yeKCtR",
        "outputId": "5cc3aabc-6e8f-4ffb-8203-2d69c0bfcc22"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['8444']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulating Dates"
      ],
      "metadata": {
        "id": "45cAbvwwwmnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"01-01\"\n",
        "datas = [\"02-03-2021\", \"12-04-1985\", \"10-10-1999\"]"
      ],
      "metadata": {
        "id": "BWyxFnbCwqPg"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}