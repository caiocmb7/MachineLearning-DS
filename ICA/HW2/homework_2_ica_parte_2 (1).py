# -*- coding: utf-8 -*-
"""Homework 2 - ICA - Parte 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I_bLa9nZA4vYE33LLmttppJ8qGpTkZrq
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
from scipy import stats
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from math import sqrt
from sklearn.metrics import r2_score

df = pd.read_csv('/content/AB_NYC_2019.csv')

df

# a curva do price_log é mais distribuida do que a do price normal, por isso a mudança

df['price_log'] = np.log(df.price+1)

"""Limpeza dos Dados"""

# tirando colunas que nao serão usadas
df = df.drop(columns=['name','id' ,'host_id','host_name', 
                                   'last_review','price'])

mean = df['reviews_per_month'].mean()
df['reviews_per_month'].fillna(mean, inplace=True)

# nosso previsor no caso será room_type, então analisando aqui quem é o menor incidente

df["room_type"].value_counts()

# retirando o menor incidente -> shared room

df.drop(df.loc[df["room_type"] == 'Shared room'].index, inplace=True)

# copiando room type para colocar na ultima coluna

df["room_types"] = df["room_type"]

# retirando room type da coluna do meio e deixando room_types no final

df.drop(labels = "room_type", axis = 1, inplace = True)

df_x = df.iloc[:,:-1]
df_x

from sklearn.preprocessing import LabelEncoder
X_neighbourhood_group = LabelEncoder()
X_neighbourhood = LabelEncoder()

df_x.iloc[:, 0] = X_neighbourhood_group.fit_transform(df_x.iloc[:, 0])
df_x.iloc[:, 1] = X_neighbourhood.fit_transform(df_x.iloc[:, 1])

df_x

Y_room_types = LabelEncoder()
df.iloc[:, -1] = Y_room_types.fit_transform(df.iloc[:, -1])
df_y = df.iloc[:,-1]
df_y

"""Normalização e Divisão"""

# normalização

scaler = StandardScaler()
df_x = scaler.fit_transform(df_x)

df_x

# divisão -> 70% treino | 30% teste

X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.3, random_state=42)

"""## Item 1

fonte: https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python

# Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

# all parameters not specified are set to their defaults
logisticRegr = LogisticRegression()

logisticRegr.fit(X_train, y_train)

# previsao
predictions = logisticRegr.predict(X_test)

# Use score method to get accuracy of model
score = logisticRegr.score(X_test, y_test)
print(score)

"""Matriz de Confusão"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
# %matplotlib inline

cm = metrics.confusion_matrix(y_test, predictions)
print(cm)

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print("Accuracy:",metrics.accuracy_score(y_test, predictions))
print("Precision:",metrics.precision_score(y_test, predictions))
print("Recall:",metrics.recall_score(y_test, predictions))

"""ROC"""

predictions_proba = logisticRegr.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  predictions_proba)
auc = metrics.roc_auc_score(y_test, predictions_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

"""## Item 2

kNN
https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/
"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 20)
classifier.fit(X_train, y_train)

predictions = classifier.predict(X_test)

cm = metrics.confusion_matrix(y_test, predictions)

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print("Accuracy:",metrics.accuracy_score(y_test, predictions))
print("Precision:",metrics.precision_score(y_test, predictions))
print("Recall:",metrics.recall_score(y_test, predictions))

"""com 5: 

Accuracy: 0.8285035961175895
Precision: 0.8150373735893303
Recall: 0.8232420429311621

com 6: 

Accuracy: 0.8244535996089658
Precision: 0.8445166531275385
Recall: 0.7695040710584752

com 7:

Accuracy: 0.8294113539557294
Precision: 0.8145608403851765
Recall: 0.8264988897113249

com 8:

Accuracy: 0.8278751483834927
Precision: 0.8377952755905512
Recall: 0.7875647668393783

com 9:

Accuracy: 0.8301794567418477
Precision: 0.8141258538003198
Recall: 0.8293116210214656

com 12:

Accuracy: 0.8311570421059982
Precision: 0.8305136412132297
Recall: 0.8066617320503331

com 13: 

Accuracy: 0.8318553173661057
Precision: 0.8131393171012823
Recall: 0.8355292376017764

com 20: 

Accuracy: 0.8352768661406327
Precision: 0.8244759374077355
Recall: 0.8267949666913398


"""

predictions_proba = classifier.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  predictions_proba)
auc = metrics.roc_auc_score(y_test, predictions_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

"""SVM"""

#Import svm model
from sklearn import svm

#Create a svm Classifier
clf = svm.SVC(kernel='linear') # Linear Kernel

#Train the model using the training sets
clf.fit(X_train, y_train)

#Predict the response for test dataset
predictions = clf.predict(X_test)

cm = metrics.confusion_matrix(y_test, predictions)

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print("Accuracy:",metrics.accuracy_score(y_test, predictions))
print("Precision:",metrics.precision_score(y_test, predictions))
print("Recall:",metrics.recall_score(y_test, predictions))

import numpy as np
import pylab as pl
from sklearn import svm
from sklearn.utils import shuffle
from sklearn.metrics import roc_curve, auc

roc_auc = auc(fpr, tpr)

pl.clf()
pl.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
pl.plot([0, 1], [0, 1], 'k--')
pl.xlim([0.0, 1.0])
pl.ylim([0.0, 1.0])
pl.xlabel('False Positive Rate')
pl.ylabel('True Positive Rate')
pl.title('Receiver operating characteristic example')
pl.legend(loc="lower right")
pl.show()

# https://go2analytics.wordpress.com/2016/07/26/implement-classification-in-python-and-roc-plotting-svc-example/

"""QDA
https://www.datasklr.com/select-classification-methods/linear-and-quadratic-discriminant-analysis
"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

#Base QDA Without any tuning
QDA_model_default = QuadraticDiscriminantAnalysis()
QDA_model_default.fit(X_train, y_train)
predictions_QDA_default = QDA_model_default.predict(X_test)

cm = metrics.confusion_matrix(y_test, predictions_QDA_default)

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print("Accuracy:",metrics.accuracy_score(y_test, predictions_QDA_default))
print("Precision:",metrics.precision_score(y_test, predictions_QDA_default))
print("Recall:",metrics.recall_score(y_test, predictions_QDA_default))

#Parameter tuning with GridSearchCV 
#######################
### QDA
#######################

estimator_3 = QuadraticDiscriminantAnalysis()
parameters_3 = {
    'reg_param': (0.00001, 0.0001, 0.001,0.01, 0.1), 
    'store_covariance': (True, False),
    'tol': (0.0001, 0.001,0.01, 0.1), 
                   }
# with GridSearch
grid_search_qda = GridSearchCV(
    estimator=estimator_3,
    param_grid=parameters_3,
    scoring = 'accuracy',
    n_jobs = -1,
    cv = 5
)
qda_1=grid_search_qda.fit(X_train, y_train)
predictions_QDA_tuning = qda_1.predict(X_test)

cm = metrics.confusion_matrix(y_test, predictions_QDA_tuning)

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print("Accuracy:",metrics.accuracy_score(y_test, predictions_QDA_tuning))
print("Precision:",metrics.precision_score(y_test, predictions_QDA_tuning))
print("Recall:",metrics.recall_score(y_test, predictions_QDA_tuning))